{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN test models.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Giogia/Machine-Learning-Project-2018-2019/blob/master/CNN_test_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyVVsSkXDKSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU7obrZkqVtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def randomize(a, b):\n",
        "    # Generate the permutation index array.\n",
        "    s = np.arange(a.shape[0])\n",
        "    np.random.shuffle(s)\n",
        "    # Shuffle the arrays by giving the permutation in the square brackets.\n",
        "    shuffled_a = a[s]\n",
        "    shuffled_b = b[s]\n",
        "    return shuffled_a, shuffled_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5PisKxnDefD",
        "colab_type": "code",
        "outputId": "7d8062c3-729d-4f7e-8c79-581c89d9e707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1856
        }
      },
      "source": [
        "results = [[],[],[]]\n",
        "\n",
        "N = 10\n",
        "\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "img_x, img_y = 28, 28\n",
        "\n",
        "input_shape=(img_x, img_y, 1)\n",
        "\n",
        "model1 = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, kernel_size=(5, 5), strides=(1, 1), input_shape=input_shape),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(tf.nn.relu),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "    keras.layers.Conv2D(64, (5, 5)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(tf.nn.relu),\n",
        "    keras.layers.SpatialDropout2D(0.5),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(tf.nn.relu),\n",
        "    keras.layers.Dense(10),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model2 = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, kernel_size=(5, 5), strides=(1, 1), input_shape=input_shape),\n",
        "    keras.layers.Activation(tf.nn.relu),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "    keras.layers.Conv2D(64, (5, 5)),\n",
        "    keras.layers.Activation(tf.nn.relu),\n",
        "    keras.layers.SpatialDropout2D(0.5),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(tf.nn.relu),\n",
        "    keras.layers.Dense(10),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model3 = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, kernel_size=(5, 5), strides=(1, 1), input_shape=input_shape),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(tf.nn.relu),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "    keras.layers.Conv2D(64, (5, 5)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(tf.nn.relu),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(tf.nn.relu),\n",
        "    keras.layers.Dense(10),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(tf.nn.softmax)\n",
        "])\n",
        "\n",
        "models = [model1,model2,model3]\n",
        "\n",
        "for i in range(N):\n",
        "  \n",
        "  (train_img, train_labels), (test_img, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "  train_img, train_labels = randomize(train_img, train_labels)\n",
        "\n",
        "\n",
        "  class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
        "                 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "  \n",
        "  train_img = train_img / 255.0\n",
        "  test_img = test_img / 255.0\n",
        "\n",
        "  limit = int(len(train_img)*0.2)\n",
        "\n",
        "  eval_img = train_img[:limit]\n",
        "  eval_labels = train_labels[:limit]\n",
        "\n",
        "  train_img = train_img[limit:]\n",
        "  train_labels = train_labels[limit:]\n",
        "\n",
        "\n",
        "  train_img = train_img.reshape(train_img.shape[0], img_x, img_y, 1)\n",
        "  eval_img = eval_img.reshape(eval_img.shape[0], img_x, img_y, 1)\n",
        "  test_img = test_img.reshape(test_img.shape[0], img_x, img_y, 1)\n",
        "  \n",
        "  for i in range(len(models)) :\n",
        "    \n",
        "    model = models[i]\n",
        "\n",
        "    model.compile(optimizer='adam', \n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "    batch_size = 128\n",
        "\n",
        "    epochs = 200\n",
        "    lr_dec = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
        "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
        "\n",
        "    logs = model.fit(train_img, train_labels,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            verbose=0,\n",
        "            validation_data=(eval_img, eval_labels),\n",
        "            callbacks=[lr_dec, early_stop]\n",
        "           )\n",
        "    \n",
        "    train_loss, train_acc = model.evaluate(train_img, train_labels)\n",
        "    eval_loss, eval_acc = model.evaluate(eval_img, eval_labels)\n",
        "    test_loss, test_acc = model.evaluate(test_img, test_labels)\n",
        "\n",
        "    results[i].append((train_loss, train_acc, eval_loss, eval_acc, test_loss, test_acc))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "48000/48000 [==============================] - 5s 111us/sample - loss: 0.0912 - acc: 0.9722\n",
            "12000/12000 [==============================] - 1s 124us/sample - loss: 0.2045 - acc: 0.9281\n",
            "10000/10000 [==============================] - 1s 129us/sample - loss: 0.2215 - acc: 0.9231\n",
            "48000/48000 [==============================] - 5s 102us/sample - loss: 0.1071 - acc: 0.9654\n",
            "12000/12000 [==============================] - 1s 107us/sample - loss: 0.2095 - acc: 0.9247\n",
            "10000/10000 [==============================] - 1s 108us/sample - loss: 0.2250 - acc: 0.9214\n",
            "48000/48000 [==============================] - 5s 108us/sample - loss: 0.0577 - acc: 0.9899\n",
            "12000/12000 [==============================] - 1s 122us/sample - loss: 0.2386 - acc: 0.9221\n",
            "10000/10000 [==============================] - 1s 122us/sample - loss: 0.2569 - acc: 0.9193\n",
            "48000/48000 [==============================] - 5s 109us/sample - loss: 0.0821 - acc: 0.9751\n",
            "12000/12000 [==============================] - 1s 123us/sample - loss: 0.1244 - acc: 0.9577\n",
            "10000/10000 [==============================] - 1s 121us/sample - loss: 0.2124 - acc: 0.9262\n",
            "48000/48000 [==============================] - 5s 104us/sample - loss: 0.0869 - acc: 0.9727\n",
            "12000/12000 [==============================] - 1s 108us/sample - loss: 0.1395 - acc: 0.9493\n",
            "10000/10000 [==============================] - 1s 108us/sample - loss: 0.2176 - acc: 0.9247\n",
            "48000/48000 [==============================] - 5s 109us/sample - loss: 0.0244 - acc: 0.9977\n",
            "12000/12000 [==============================] - 1s 124us/sample - loss: 0.1119 - acc: 0.9657\n",
            "10000/10000 [==============================] - 1s 128us/sample - loss: 0.2648 - acc: 0.9217\n",
            "48000/48000 [==============================] - 5s 110us/sample - loss: 0.0616 - acc: 0.9826\n",
            "12000/12000 [==============================] - 2s 129us/sample - loss: 0.1226 - acc: 0.9573\n",
            "10000/10000 [==============================] - 1s 125us/sample - loss: 0.2166 - acc: 0.9258\n",
            "48000/48000 [==============================] - 5s 105us/sample - loss: 0.0657 - acc: 0.9802\n",
            "12000/12000 [==============================] - 1s 113us/sample - loss: 0.1315 - acc: 0.9526\n",
            "10000/10000 [==============================] - 1s 112us/sample - loss: 0.2169 - acc: 0.9272\n",
            "48000/48000 [==============================] - 5s 111us/sample - loss: 0.0078 - acc: 0.9999\n",
            "12000/12000 [==============================] - 2s 130us/sample - loss: 0.0908 - acc: 0.9693\n",
            "10000/10000 [==============================] - 1s 128us/sample - loss: 0.2966 - acc: 0.9200\n",
            "48000/48000 [==============================] - 6s 115us/sample - loss: 0.0545 - acc: 0.9846\n",
            "12000/12000 [==============================] - 2s 130us/sample - loss: 0.0920 - acc: 0.9687\n",
            "10000/10000 [==============================] - 1s 129us/sample - loss: 0.2186 - acc: 0.9249\n",
            "48000/48000 [==============================] - 5s 106us/sample - loss: 0.0635 - acc: 0.9811\n",
            "12000/12000 [==============================] - 1s 117us/sample - loss: 0.1000 - acc: 0.9643\n",
            "10000/10000 [==============================] - 1s 120us/sample - loss: 0.2155 - acc: 0.9254\n",
            "48000/48000 [==============================] - 6s 116us/sample - loss: 0.0039 - acc: 1.0000\n",
            "12000/12000 [==============================] - 2s 138us/sample - loss: 0.0423 - acc: 0.9869\n",
            "10000/10000 [==============================] - 1s 139us/sample - loss: 0.3185 - acc: 0.9187\n",
            "48000/48000 [==============================] - 6s 116us/sample - loss: 0.0449 - acc: 0.9879\n",
            "12000/12000 [==============================] - 2s 134us/sample - loss: 0.0851 - acc: 0.9718\n",
            "10000/10000 [==============================] - 1s 135us/sample - loss: 0.2207 - acc: 0.9261\n",
            "48000/48000 [==============================] - 5s 109us/sample - loss: 0.0569 - acc: 0.9835\n",
            "12000/12000 [==============================] - 1s 123us/sample - loss: 0.0919 - acc: 0.9685\n",
            "10000/10000 [==============================] - 1s 120us/sample - loss: 0.2164 - acc: 0.9281\n",
            "48000/48000 [==============================] - 6s 118us/sample - loss: 0.0021 - acc: 1.0000\n",
            "12000/12000 [==============================] - 2s 134us/sample - loss: 0.0226 - acc: 0.9939\n",
            "10000/10000 [==============================] - 1s 136us/sample - loss: 0.3383 - acc: 0.9206\n",
            "48000/48000 [==============================] - 6s 117us/sample - loss: 0.0396 - acc: 0.9897\n",
            "12000/12000 [==============================] - 2s 133us/sample - loss: 0.0687 - acc: 0.9783\n",
            "10000/10000 [==============================] - 1s 135us/sample - loss: 0.2215 - acc: 0.9260\n",
            "48000/48000 [==============================] - 6s 118us/sample - loss: 0.0398 - acc: 0.9899\n",
            "12000/12000 [==============================] - 2s 128us/sample - loss: 0.0788 - acc: 0.9727\n",
            "10000/10000 [==============================] - 1s 128us/sample - loss: 0.2205 - acc: 0.9313\n",
            "48000/48000 [==============================] - 6s 124us/sample - loss: 0.0011 - acc: 1.0000\n",
            "12000/12000 [==============================] - 2s 136us/sample - loss: 0.0114 - acc: 0.9971\n",
            "10000/10000 [==============================] - 1s 138us/sample - loss: 0.3625 - acc: 0.9187\n",
            "48000/48000 [==============================] - 6s 123us/sample - loss: 0.0271 - acc: 0.9943\n",
            "12000/12000 [==============================] - 2s 137us/sample - loss: 0.0775 - acc: 0.9734\n",
            "10000/10000 [==============================] - 1s 139us/sample - loss: 0.2293 - acc: 0.9252\n",
            "48000/48000 [==============================] - 6s 124us/sample - loss: 0.0358 - acc: 0.9914\n",
            "12000/12000 [==============================] - 2s 132us/sample - loss: 0.0687 - acc: 0.9756\n",
            "10000/10000 [==============================] - 1s 133us/sample - loss: 0.2219 - acc: 0.9297\n",
            "48000/48000 [==============================] - 6s 123us/sample - loss: 8.9097e-04 - acc: 1.0000\n",
            "12000/12000 [==============================] - 2s 136us/sample - loss: 0.0092 - acc: 0.9976\n",
            "10000/10000 [==============================] - 1s 137us/sample - loss: 0.3712 - acc: 0.9217\n",
            "48000/48000 [==============================] - 6s 126us/sample - loss: 0.0251 - acc: 0.9944\n",
            "12000/12000 [==============================] - 2s 138us/sample - loss: 0.0521 - acc: 0.9837\n",
            "10000/10000 [==============================] - 1s 137us/sample - loss: 0.2282 - acc: 0.9259\n",
            "48000/48000 [==============================] - 6s 126us/sample - loss: 0.0307 - acc: 0.9933\n",
            "12000/12000 [==============================] - 2s 137us/sample - loss: 0.0707 - acc: 0.9751\n",
            "10000/10000 [==============================] - 1s 138us/sample - loss: 0.2285 - acc: 0.9300\n",
            "48000/48000 [==============================] - 6s 129us/sample - loss: 3.8934e-04 - acc: 1.0000\n",
            "12000/12000 [==============================] - 2s 128us/sample - loss: 0.0060 - acc: 0.9986\n",
            "10000/10000 [==============================] - 1s 128us/sample - loss: 0.4020 - acc: 0.9189\n",
            "48000/48000 [==============================] - 6s 132us/sample - loss: 0.0230 - acc: 0.9954\n",
            "12000/12000 [==============================] - 2s 142us/sample - loss: 0.0437 - acc: 0.9861\n",
            "10000/10000 [==============================] - 1s 144us/sample - loss: 0.2301 - acc: 0.9271\n",
            "48000/48000 [==============================] - 6s 131us/sample - loss: 0.0257 - acc: 0.9945\n",
            "12000/12000 [==============================] - 2s 138us/sample - loss: 0.0491 - acc: 0.9843\n",
            "10000/10000 [==============================] - 1s 139us/sample - loss: 0.2301 - acc: 0.9295\n",
            "48000/48000 [==============================] - 7s 139us/sample - loss: 5.3140e-04 - acc: 1.0000\n",
            "12000/12000 [==============================] - 2s 148us/sample - loss: 0.0064 - acc: 0.9983\n",
            "10000/10000 [==============================] - 1s 146us/sample - loss: 0.4047 - acc: 0.9198\n",
            "48000/48000 [==============================] - 7s 139us/sample - loss: 0.0181 - acc: 0.9970\n",
            "12000/12000 [==============================] - 2s 143us/sample - loss: 0.0373 - acc: 0.9884\n",
            "10000/10000 [==============================] - 1s 142us/sample - loss: 0.2362 - acc: 0.9286\n",
            "48000/48000 [==============================] - 6s 132us/sample - loss: 0.0226 - acc: 0.9957\n",
            "12000/12000 [==============================] - 2s 140us/sample - loss: 0.0424 - acc: 0.9862\n",
            "10000/10000 [==============================] - 1s 139us/sample - loss: 0.2340 - acc: 0.9300\n",
            "48000/48000 [==============================] - 7s 152us/sample - loss: 2.2718e-04 - acc: 1.0000\n",
            "12000/12000 [==============================] - 2s 153us/sample - loss: 0.0045 - acc: 0.9990\n",
            "10000/10000 [==============================] - 1s 149us/sample - loss: 0.4270 - acc: 0.9182\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dufJsucEKMd",
        "colab_type": "code",
        "outputId": "3bd12f22-8baa-4856-bdcf-723700a32a3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "finals = [[],[],[]]\n",
        "\n",
        "for i in range(3):\n",
        "  \n",
        "  for k in range(len(results[i][0])):\n",
        "    \n",
        "      final = 0\n",
        "  \n",
        "      for j in range(N):\n",
        "  \n",
        "        final += results[i][j][k]/N\n",
        "    \n",
        "      finals[i].append(final)\n",
        "      \n",
        "print(\"train_loss            train_acc           eval_loss           eval_acc             test_loss           test_acc\")\n",
        "\n",
        "for i in range(3):      \n",
        "  \n",
        "  print(finals[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_loss            train_acc           eval_loss           eval_acc             test_loss           test_acc\n",
            "[0.04669433649538745, 0.987325006723404, 0.09080164908173805, 0.9693333446979523, 0.22352502596579493, 0.9258899986743927]\n",
            "[0.0534778334485561, 0.9847666680812837, 0.09821340495795013, 0.965316665172577, 0.22263487272530794, 0.9277300059795379]\n",
            "[0.009919683997391258, 0.9987583339214323, 0.054369948742508604, 0.9828499972820283, 0.3442438995930366, 0.9197600007057191]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}